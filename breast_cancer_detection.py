# -*- coding: utf-8 -*-
"""breast_cancer_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZEtAfcSFE2aYWKoK3af75h9VLx_e6ldg

DETECTING BREAST CANCER
"""

#import the libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#loading the data
from  google.colab import files
uploaded = files.upload()
df=pd.read_csv('data.csv')
df.head(7)

#num of row and columns
df.shape

#count numm of empty(Nan, nan, na) values in each column
df.isnull().sum()

#drop column with missing values
df=df.dropna(axis=1)

#get count of num of malignant(M) and begign(B)
df['diagnosis'].value_counts()

#Visualise the count
sns.countplot(df['diagnosis'], label='count')

#looking at the data types, to see the columns to be encoded
df.dtypes

#encode categorical data values
from sklearn.preprocessing import LabelEncoder
l_encoder=LabelEncoder()
df.iloc[:,1]=l_encoder.fit_transform(df.iloc[:,1].values)
df.iloc[:,1]

#create a pairplot
sns.pairplot(df.iloc[:,1:7], hue='diagnosis')

df.head()

#get the correlation
df.iloc[:,1:12].corr

#visualise the correlation
plt.figure(figsize=(10,10))
sns.heatmap(df.iloc[:,1:12].corr(), annot=True, fmt='.0%')

#split the data inot independent(X) and dependent(Y) datasets
X=df.iloc[:,2:31].values
Y=df.iloc[:,1].values

#splitting the dataset in 75%training and 25% testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.25, random_state=0)

#scale the data(feature scaling)
from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

#create a funtion for the models
def models(X_train, Y_train):

  #logistic regression
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state=0)
  log.fit(X_train, Y_train)

  #decision tree
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion="entropy", random_state=0)
  tree.fit(X_train, Y_train)

  #randomforrestclassifier
  from sklearn.ensemble import RandomForestClassifier
  forest=RandomForestClassifier(n_estimators=10, criterion="entropy", random_state=0)
  forest.fit(X_train, Y_train)

  #print the model'S accuracy on the training data
  print('[0]Logistic Training Accuracy:', log.score(X_train, Y_train))
  print('[1]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train))
  print('[2]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train))

  return log, tree, forest

model=models(X_train, Y_train)

#test model accuracy on test data on confusion matrix
from sklearn.metrics import confusion_matrix
for i in range(len(model)):
  print('Model',i)
  cm = confusion_matrix(Y_test, model[i].predict(X_test))

  TP=cm[0][0]
  TN=cm[1][1]
  FP=cm[0][1]
  FN=cm[1][0]
  acc = (TP+TN)/(TP+TN+FP+FN)
  
  print(cm)
  print(f'Testing Accuracy is {acc}''\n''')

#another way to get metrics of the models
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range(len(model)):
  print('Model', i)
  print(classification_report(Y_test, model[i].predict(X_test)),'\n')
  print(accuracy_score(Y_test, model[i].predict(X_test)),'\n')

#print the prediction of the Random Forest Classifier model
pred = model[2].predict(X_test)
print(pred,'\n')
print(Y_test)

